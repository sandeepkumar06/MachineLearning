import pandas as pd 
from sklearn import linear_model
from sklearn.model_selection import train_test_split
from sklearn.decomposition import PCA

import matplotlib.pyplot as plt

dataset = pd.read_csv('signaldataset3.csv')
df_x = dataset.iloc[:, 0:10].values
df_y = dataset.iloc[:, 10].values
df_y=df_y.astype('int')

print(dataset)

reg = linear_model.LinearRegression()
x_train,x_test,y_train,y_test = train_test_split(df_x,df_y,test_size=0.2,random_state=4)
reg.fit(x_train,y_train)
print(reg.score(x_test,y_test))

print(df_x.shape)

#applying PCA
pca = PCA(n_components=2,whiten='True')
x = pca.fit(df_x).transform(df_x)
print (x)
# Print the number of features
print('Original number of features: %s ', str(df_x.shape[1]))
print('Reduced number of features: %s', str(x.shape[1]))

## View the ratio of explained variance
print('explained variance ratio : %s ', str(pca.explained_variance_ratio_))

print(pca.explained_variance_)
X_new = pca.inverse_transform(x)
plt.scatter(df_x[:, 0], df_x[:, 1],s=7,alpha=0.2)
plt.scatter(X_new[:, 0], X_new[:, 1],s=7, alpha=0.8)
plt.axis('equal');
plt.title('Plotting of data using PCA components')
plt.show()  
#finding Linear Regression after PCA

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
# Fit on training set only.
scaler.fit(x_train)
# Apply transform to both the training set and the test set.
x_train = scaler.transform(x_train)
x_test = scaler.transform(x_test)
reg = linear_model.LinearRegression()

reg.fit(x_train,y_train)

print(reg.score(x_test,y_test))
print(reg.predict(x_test))
# Make an instance of the Model
pca = PCA(.95)
pca.fit(x_train)
print (pca.n_components_)

x_train = pca.transform(x_train)
x_test = pca.transform(x_test)

#finding Linear Regression after PCA
reg = linear_model.LinearRegression()

reg.fit(x_train,y_train)

print(reg.score(x_test,y_test))
print(reg.predict(x_test))
